{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Projects markdown generator for academicpages\n",
    "\n",
    "Takes a TSV of projects with metadata and converts them for use with [academicpages.github.io](academicpages.github.io). This is an interactive Jupyter notebook ([see more info here](http://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html)). \n",
    "\n",
    "TODO: Make this work with BibTex and other databases, rather than Stuart's non-standard TSV format and citation style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data format\n",
    "\n",
    "The TSV needs to have the following columns: title, type, url_slug, venue, date, location, slides_url, video_url, excerpt, with a header at the top. Many of these fields can be blank, but the columns must be in the TSV.\n",
    "\n",
    "- Fields that cannot be blank: `title`, `url_slug`, `date`. All else can be blank. `type` defaults to \"Talk\" \n",
    "- `date` must be formatted as YYYY-MM-DD.\n",
    "- `url_slug` will be the descriptive part of the .md file and the permalink URL for the page about the paper. \n",
    "    - The .md file will be `YYYY-MM-DD-[url_slug].md` and the permalink will be `https://[yourdomain]/talks/YYYY-MM-DD-[url_slug]`\n",
    "    - The combination of `url_slug` and `date` must be unique, as it will be the basis for your filenames\n",
    "\n",
    "This is how the raw file looks (it doesn't look pretty, use a spreadsheet or other program to edit and create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"title\": \"Extending Whisper, OpenAI's Speech-to-Text Model\",\n",
      "        \"contributors\": \"Abhishek Divekar, Yosub Jung, Roshni Tayal\",\n",
      "        \"type\": \"Academic\",\n",
      "        \"project_date\": \"2022-12-05\", \n",
      "        \"url_slug\": \"ut-mscs-case-studies-in-ml-project\", \n",
      "        \"teaser_url\": \"whisper-wer-noisy-vs-quiet.png\",\n",
      "        \"report_url\": \"https://adivekar-utexas.github.io/files/UTCS-ML-Case-Studies-Extending-Whisper-OpenAIs-Speech-to-Text-Model.pdf\",\n",
      "        \"code_url\": \"\",\n",
      "        \"excerpt\": \"We study the performance of OpenAI's Whisper model, the state-of-the-art Speech-to-text model, in noisy urban environments. To do so, we create a dataset consisting of 134 minutes of us reading out loud in both quiet and noisy urban environments (subway, street and cafe) and manually annotating the recordings at 30 second intervals. Using a powerful multi-GPU AWS cluster and distributed computing framework Ray, we find that Whisper performs significantly worse on speeches recorded in noisy environments than on those recorded in quiet environments, in contrast to assertions made by Whisper authors. This performance gap is particularly severe for small Whisper models. This finding is concerning since the small models, due to its low inference-time, are most likely to be deployed on handheld devices (like smartphones), and thus more likely to be exposed to outside noise that can degrade speech-to-text performance. To improve performance, we fine-tune the HuggingFace Whisper implementation on a split of our collected data. We find that fine-tuning on single-speaker noisy speech improves average Word Error Rate (WER) by 2.81 (from 28.76 to 25.95) and fine-tuning on multi-speaker noisy speech improves average WER by 2.61 (from 28.76 to 26.15). Thus we are able to successfully adapt OpenAI Whisper to function reliably in noisy urban environments.\",\n",
      "        \"my_contribution\": \"Helped formulate research question, recorded and annotated speeches, coded and fine-tuned hundreds of HuggingFace Whisper models uinsg Ray distributed computing framework, tabulated results, minor rewriting.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Asking the Right Questions: Question Paraphrasing Using Cross-Domain Abstractive Summarization and Backtranslation\",\n",
      "        \"contributors\": \"Abhishek Divekar, Alex Stoken\",\n",
      "        \"type\": \"Academic\",\n",
      "        \"project_date\": \"2021-05-06\", \n",
      "        \"url_slug\": \"ut-mscs-nlp-project\", \n",
      "        \"teaser_url\": \"asking-the-right-questions-table.png\",\n",
      "        \"report_url\": \"https://adivekar-utexas.github.io/files/UTCS-NLP-Question-Paraphrasing-Using-Cross-Domain-Abstractive-Summarization-and-Backtranslation.pdf\",\n",
      "        \"code_url\": \"\",\n",
      "        \"excerpt\": \"A common issue when asking questions is that they might be prone to misinterpretation: most of us have experienced when a colleague or teacher misinterprets a question and provides an answer which is tangential to the information we desire, or incomplete. This problem is exacerbated over text, where visual and emotion cues are not transmittable. We hypothesize that question answering models face the same issues as the human responder in such situations: when asked an ambiguous question, they might be unsure what to retrieve from the given passage. We propose paraphrasing the question with pre-trained language models, to improve answer retrieval and robustness to ambiguous questions. We introduce a new scoring metric, GROK, to evaluate and select good paraphrases. We show that this metric improved upon paraphrase selection via beam search for downstream tasks, and that this metric combined with data augmentation via backtranslation increases question answering performance on the NewsQA and BioASQ datasets, improving EM by 2.5% and F1 by 1.9% over-and-above the baseline on the latter.\",\n",
      "        \"my_contribution\": \"Formulated research question and GROK metric, wrote code to generate abstractive summaries and ran experiments to train hundreds of BiLSTM models on AWS GPUs.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Autonomous agents for realtime multiplayer ice-hockey\",\n",
      "        \"contributors\": \"Abhishek Divekar, Jason Housman, Ankita Sinha, Alex Stoken\",\n",
      "        \"type\": \"Academic\",\n",
      "        \"project_date\": \"2020-12-14\", \n",
      "        \"url_slug\": \"ut-mscs-deep-learning-project\", \n",
      "        \"teaser_url\": \"autonomous-agents-ice-hockey.png\",\n",
      "        \"report_url\": \"https://adivekar-utexas.github.io/files/UTCS-Deep-Learning-Final-Autonomous-agents-for-realtime-multiplayer-ice-hockey.pdf\",\n",
      "        \"code_url\": \"\",\n",
      "        \"excerpt\": \"We design an automated agent to play 2-on-2 games in SuperTuxKart IceHockey. Our two-stage system composes of a \\\"vision\\\" stage which takes as input the image of the player's Field of View and predicts world-state attributes. For vision, we train a multi-task CenterNet model (with U-Net backend), to predict whether hockey puck was on-screen (classification), puck's x-y coordinates (aimpoint regression) and distance from player (regression). These are consumed by a \\\"controller\\\" stage which return actions that update the world-state by \\\"dribbling\\\" puck towards goal, or defending against the opposing AI team.\",\n",
      "        \"my_contribution\": \"Generated training dataset of 40k images, coded and trained multi-task CenterNet model for vision stage of pipeline, wrote sections of report.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"SearchDistribute: webscraping search results on an academic budget\",\n",
      "        \"contributors\": \"Abhishek Divekar\",\n",
      "        \"type\": \"Academic\",\n",
      "        \"project_date\": \"2017-09-08\", \n",
      "        \"url_slug\": \"vjti-search-distribute\", \n",
      "        \"teaser_url\": \"\",\n",
      "        \"report_url\": \"\",\n",
      "        \"code_url\": \"https://github.com/ARDivekar/SearchDistribute\",\n",
      "        \"excerpt\": \"retrieves upto 250,000 Search engine results per day. With a $5/month VPN subscription, can extract upto 10,000+ search results per query per hour (120x cheaper than Google Search API). Built using Python and Selenium, coordinates multiple PhantomJS browser instances, each connected to a SOCKS5 proxy.\",\n",
      "        \"my_contribution\": \"Sole contributor.\"\n",
      "    }\n",
      "]"
     ]
    }
   ],
   "source": [
    "!cat projects.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import TSV\n",
    "\n",
    "Pandas makes this easy with the read_csv function. We are using a TSV, so we specify the separator as a tab, or `\\t`.\n",
    "\n",
    "I found it important to put this data in a tab-separated values format, because there are a lot of commas in this kind of data and comma-separated values can get messed up. However, you can modify the import statement, as pandas also has read_excel(), read_json(), and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>contributors</th>\n",
       "      <th>type</th>\n",
       "      <th>project_date</th>\n",
       "      <th>url_slug</th>\n",
       "      <th>teaser_url</th>\n",
       "      <th>report_url</th>\n",
       "      <th>code_url</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>my_contribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Extending Whisper, OpenAI's Speech-to-Text Model</td>\n",
       "      <td>Abhishek Divekar, Yosub Jung, Roshni Tayal</td>\n",
       "      <td>Academic</td>\n",
       "      <td>2022-12-05</td>\n",
       "      <td>ut-mscs-case-studies-in-ml-project</td>\n",
       "      <td>whisper-wer-noisy-vs-quiet.png</td>\n",
       "      <td>https://adivekar-utexas.github.io/files/UTCS-M...</td>\n",
       "      <td></td>\n",
       "      <td>We study the performance of OpenAI's Whisper m...</td>\n",
       "      <td>Helped formulate research question, recorded a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Asking the Right Questions: Question Paraphras...</td>\n",
       "      <td>Abhishek Divekar, Alex Stoken</td>\n",
       "      <td>Academic</td>\n",
       "      <td>2021-05-06</td>\n",
       "      <td>ut-mscs-nlp-project</td>\n",
       "      <td>asking-the-right-questions-table.png</td>\n",
       "      <td>https://adivekar-utexas.github.io/files/UTCS-N...</td>\n",
       "      <td></td>\n",
       "      <td>A common issue when asking questions is that t...</td>\n",
       "      <td>Formulated research question and GROK metric, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Autonomous agents for realtime multiplayer ice...</td>\n",
       "      <td>Abhishek Divekar, Jason Housman, Ankita Sinha,...</td>\n",
       "      <td>Academic</td>\n",
       "      <td>2020-12-14</td>\n",
       "      <td>ut-mscs-deep-learning-project</td>\n",
       "      <td>autonomous-agents-ice-hockey.png</td>\n",
       "      <td>https://adivekar-utexas.github.io/files/UTCS-D...</td>\n",
       "      <td></td>\n",
       "      <td>We design an automated agent to play 2-on-2 ga...</td>\n",
       "      <td>Generated training dataset of 40k images, code...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SearchDistribute: webscraping search results o...</td>\n",
       "      <td>Abhishek Divekar</td>\n",
       "      <td>Academic</td>\n",
       "      <td>2017-09-08</td>\n",
       "      <td>vjti-search-distribute</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://github.com/ARDivekar/SearchDistribute</td>\n",
       "      <td>retrieves upto 250,000 Search engine results p...</td>\n",
       "      <td>Sole contributor.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Extending Whisper, OpenAI's Speech-to-Text Model   \n",
       "1  Asking the Right Questions: Question Paraphras...   \n",
       "2  Autonomous agents for realtime multiplayer ice...   \n",
       "3  SearchDistribute: webscraping search results o...   \n",
       "\n",
       "                                        contributors      type project_date  \\\n",
       "0         Abhishek Divekar, Yosub Jung, Roshni Tayal  Academic   2022-12-05   \n",
       "1                      Abhishek Divekar, Alex Stoken  Academic   2021-05-06   \n",
       "2  Abhishek Divekar, Jason Housman, Ankita Sinha,...  Academic   2020-12-14   \n",
       "3                                   Abhishek Divekar  Academic   2017-09-08   \n",
       "\n",
       "                             url_slug                            teaser_url  \\\n",
       "0  ut-mscs-case-studies-in-ml-project        whisper-wer-noisy-vs-quiet.png   \n",
       "1                 ut-mscs-nlp-project  asking-the-right-questions-table.png   \n",
       "2       ut-mscs-deep-learning-project      autonomous-agents-ice-hockey.png   \n",
       "3              vjti-search-distribute                                         \n",
       "\n",
       "                                          report_url  \\\n",
       "0  https://adivekar-utexas.github.io/files/UTCS-M...   \n",
       "1  https://adivekar-utexas.github.io/files/UTCS-N...   \n",
       "2  https://adivekar-utexas.github.io/files/UTCS-D...   \n",
       "3                                                      \n",
       "\n",
       "                                        code_url  \\\n",
       "0                                                  \n",
       "1                                                  \n",
       "2                                                  \n",
       "3  https://github.com/ARDivekar/SearchDistribute   \n",
       "\n",
       "                                             excerpt  \\\n",
       "0  We study the performance of OpenAI's Whisper m...   \n",
       "1  A common issue when asking questions is that t...   \n",
       "2  We design an automated agent to play 2-on-2 ga...   \n",
       "3  retrieves upto 250,000 Search engine results p...   \n",
       "\n",
       "                                     my_contribution  \n",
       "0  Helped formulate research question, recorded a...  \n",
       "1  Formulated research question and GROK metric, ...  \n",
       "2  Generated training dataset of 40k images, code...  \n",
       "3                                  Sole contributor.  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects = pd.read_json(\"projects.json\")\n",
    "projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escape special characters\n",
    "\n",
    "YAML is very picky about how it takes a valid string, so we are replacing single and double quotes (and ampersands) with their HTML encoded equivilents. This makes them look not so readable in raw format, but they are parsed and rendered nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "html_escape_table = {\n",
    "    \"&\": \"&amp;\",\n",
    "    '\"': \"&quot;\",\n",
    "    \"'\": \"&apos;\"\n",
    "    }\n",
    "\n",
    "def html_escape(text):\n",
    "    if type(text) is str:\n",
    "        return \"\".join(html_escape_table.get(c,c) for c in text)\n",
    "    else:\n",
    "        return \"False\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the markdown files\n",
    "\n",
    "This is where the heavy lifting is done. This loops through all the rows in the TSV dataframe, then starts to concatentate a big string (```md```) that contains the markdown for each type. It does the YAML metadata first, then does the description for the individual page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loc_dict = {}\n",
    "\n",
    "for row, item in projects.iterrows():\n",
    "    \n",
    "    md_filename = str(item.project_date) + \"-\" + item.url_slug + \".md\"\n",
    "    html_filename = str(item.project_date) + \"-\" + item.url_slug \n",
    "    year = item.project_date[:4]\n",
    "    \n",
    "    md = \"---\\ntitle: \\\"\"   + item.title + '\"\\n'\n",
    "    md += \"collection: projects\" + \"\\n\"\n",
    "    md += f'urlslug: \"{item.url_slug }\"\\n'\n",
    "    if len(str(item.type)) > 3:\n",
    "        md += 'type: \"' + item.type + '\"\\n'\n",
    "    else:\n",
    "        md += 'type: \"Project\"\\n'\n",
    "    md += \"permalink: /projects/\" + html_filename + \"\\n\"\n",
    "    if len(str(item.contributors)) > 3:\n",
    "        md += 'contributors: \"' + item.contributors + '\"\\n'\n",
    "    if len(str(item.my_contribution)) > 3:\n",
    "        md += 'contribution: \"' + item.my_contribution + '\"\\n'\n",
    "    md += \"date: \" + str(item.project_date) + \"\\n\"\n",
    "    if len(str(item.teaser_url)) > 5:\n",
    "        md += \"teaserurl: '\" + item.teaser_url + \"'\\n\"\n",
    "    report_url = None\n",
    "    if len(str(item.report_url)) > 5:\n",
    "        report_url = str(item.report_url)\n",
    "        md += \"reporturl: '\" + item.report_url + \"'\\n\"\n",
    "    code_url = None\n",
    "    if len(str(item.code_url)) > 5:\n",
    "        code_url = str(item.code_url)\n",
    "        md += \"codeurl: '\" + item.code_url + \"'\\n\"\n",
    "    if len(str(item.excerpt)) > 5:\n",
    "        md += \"excerpt: '\" + html_escape(item.excerpt) + \"'\\n\"\n",
    "           \n",
    "    md += \"---\\n\"\n",
    "\n",
    "    more_info = []\n",
    "    if code_url is not None:\n",
    "        more_info.append(f'[[Code]({code_url})]')\n",
    "    if report_url is not None:\n",
    "        more_info.append(f'[[Technical report]({report_url})]')\n",
    "    if len(item.contributors) > 0:\n",
    "        md += f\"\\n{item.contributors}\\n\"\n",
    "    if len(str(item.excerpt)) > 4:\n",
    "        md += f\"\\n**Description:**\\n{html_escape(item.excerpt)}\\n\"\n",
    "    if len(str(item.my_contribution)) > 4:\n",
    "        md += f\"\\n**My contribution:**\\n{html_escape(item.my_contribution)}\\n\"\n",
    "    if len(more_info) > 0:\n",
    "        md += f\"\\n**Resources:** {' '.join(more_info)}\\n\"\n",
    "        \n",
    "    md_filename = os.path.basename(md_filename)\n",
    "    #print(md)\n",
    "    \n",
    "    with open(\"../_projects/\" + md_filename, 'w') as f:\n",
    "        f.write(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are in the talks directory, one directory below where we're working from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-08-vjti-search-distribute.md\n",
      "2020-12-14-ut-mscs-deep-learning-project.md\n",
      "2021-05-06-ut-mscs-nlp-project.md\n",
      "2022-12-05-ut-mscs-case-studies-in-ml-project.md\n"
     ]
    }
   ],
   "source": [
    "!ls ../_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: \"Autonomous agents for realtime multiplayer ice-hockey\"\n",
      "collection: projects\n",
      "urlslug: \"ut-mscs-deep-learning-project\"\n",
      "type: \"Academic\"\n",
      "permalink: /projects/2020-12-14-ut-mscs-deep-learning-project\n",
      "contributors: \"Abhishek Divekar, Jason Housman, Ankita Sinha, Alex Stoken\"\n",
      "contribution: \"Generated training dataset of 40k images, coded and trained multi-task CenterNet model for vision stage of pipeline, wrote sections of report.\"\n",
      "date: 2020-12-14\n",
      "teaserurl: 'autonomous-agents-ice-hockey.png'\n",
      "reporturl: 'https://adivekar-utexas.github.io/files/UTCS-Deep-Learning-Final-Autonomous-agents-for-realtime-multiplayer-ice-hockey.pdf'\n",
      "excerpt: 'We design an automated agent to play 2-on-2 games in SuperTuxKart IceHockey. Our two-stage system composes of a &quot;vision&quot; stage which takes as input the image of the player&apos;s Field of View and predicts world-state attributes. For vision, we train a multi-task CenterNet model (with U-Net backend), to predict whether hockey puck was on-screen (classification), puck&apos;s x-y coordinates (aimpoint regression) and distance from player (regression). These are consumed by a &quot;controller&quot; stage which return actions that update the world-state by &quot;dribbling&quot; puck towards goal, or defending against the opposing AI team.'\n",
      "---\n",
      "\n",
      "Abhishek Divekar, Jason Housman, Ankita Sinha, Alex Stoken\n",
      "\n",
      "**Description:**\n",
      "We design an automated agent to play 2-on-2 games in SuperTuxKart IceHockey. Our two-stage system composes of a &quot;vision&quot; stage which takes as input the image of the player&apos;s Field of View and predicts world-state attributes. For vision, we train a multi-task CenterNet model (with U-Net backend), to predict whether hockey puck was on-screen (classification), puck&apos;s x-y coordinates (aimpoint regression) and distance from player (regression). These are consumed by a &quot;controller&quot; stage which return actions that update the world-state by &quot;dribbling&quot; puck towards goal, or defending against the opposing AI team.\n",
      "\n",
      "**My contribution:**\n",
      "Generated training dataset of 40k images, coded and trained multi-task CenterNet model for vision stage of pipeline, wrote sections of report.\n",
      "\n",
      "**Resources:** [[Technical report](https://adivekar-utexas.github.io/files/UTCS-Deep-Learning-Final-Autonomous-agents-for-realtime-multiplayer-ice-hockey.pdf)]\n"
     ]
    }
   ],
   "source": [
    "!cat ../_projects/2020-12-14-ut-mscs-deep-learning-project.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('ghpages')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "06f7e23954b94c5e9c6c2692449a9adafbf148265f2ef39a8350fb6de62c0ffb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
